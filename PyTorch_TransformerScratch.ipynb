{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45310a0c",
      "metadata": {
        "id": "45310a0c"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Transformers have revolutionized the field of natural language processing.\n",
        "\n",
        "Transformers are basically deep learning models introduced in the paper \"Attention Is All You Need\" by Vaswani et al. They have since become the state-of-the-art model for a range of tasks in natural language processing (NLP). Unlike previous models which relied heavily on RNNs or CNNs, Transformers use self-attention mechanisms to weigh the significance of different words in a sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1de3524",
      "metadata": {
        "id": "c1de3524"
      },
      "source": [
        "# Objective\n",
        "\n",
        "The main goal of this notebook is to implement the Transformer architecture from scratch, understand its core components, and test its capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qXuBQyZ1yBdU",
      "metadata": {
        "id": "qXuBQyZ1yBdU"
      },
      "source": [
        "The objective of this notebook is to implement a Transformer model from scratch. Building it step by step allows us to understand the intricacies involved and the reasoning behind each design choice. By the end of this notebook, we aim to have a working model that can be trained and evaluated on NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eccaf4b0",
      "metadata": {
        "id": "eccaf4b0"
      },
      "source": [
        "# Setting Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fJf4VAnnyYNk",
      "metadata": {
        "id": "fJf4VAnnyYNk"
      },
      "source": [
        "In this section, we'll set up our environment by importing the necessary libraries and defining any configurations required for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "351988b2",
      "metadata": {
        "id": "351988b2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168603c6",
      "metadata": {
        "id": "168603c6"
      },
      "source": [
        "# Implementing the Self-Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSE9W0t1ye5r",
      "metadata": {
        "id": "eSE9W0t1ye5r"
      },
      "source": [
        "The self-attention mechanism is at the heart of the Transformer architecture. It allows the model to focus on different words in the input when producing an output, assigning different attention scores to different words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b1cadf12",
      "metadata": {
        "id": "b1cadf12"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        # Ensure the embedding size is divisible by number of heads\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        # Linear layers for the queries, keys, and values\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # Output fully connected layer\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Transform the input using the query, key, and value linear layers\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(query)\n",
        "\n",
        "        # Split the embedding size into the number of heads\n",
        "        # This allows for multiple attention scores\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        # Scaled dot-product attention mechanism\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Turn the energy values into probabilities ranging from 0 to 1\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        # Multiply the attention scores with the values to get the final output\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        # Apply the output linear layer\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "9C5a9-BkDKwu"
      },
      "id": "9C5a9-BkDKwu",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4fcf342",
      "metadata": {
        "id": "e4fcf342"
      },
      "source": [
        "# Building the Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S1i1qkFpyoP6",
      "metadata": {
        "id": "S1i1qkFpyoP6"
      },
      "source": [
        "The encoder processes the input sequence and compresses this information into a 'context' or 'memory' that the decoder can then use. Each encoder consists of multiple layers of self-attention and feed-forward neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3b370c40",
      "metadata": {
        "id": "3b370c40"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Set the embedding size and device\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "\n",
        "        # Create word and position embeddings\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        # Initialize encoder layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Get number of training examples and sequence length\n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        # Create position embeddings\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "        # Combine word embeddings and position embeddings\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # Pass the input through the encoder layers\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5d7389",
      "metadata": {
        "id": "ed5d7389"
      },
      "source": [
        "# Constructing the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hH-T3H3oyt0c",
      "metadata": {
        "id": "hH-T3H3oyt0c"
      },
      "source": [
        "The decoder generates the output sequence. It uses the context provided by the encoder to produce the correct output. Similar to the encoder, it has multiple layers, but in addition to the self-attention mechanism, it also has an encoder-decoder attention layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5c5d00c",
      "metadata": {
        "id": "f5c5d00c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # Self attention mechanism for the decoder block\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "\n",
        "        # Transformer block which includes another self attention mechanism followed by a feed-forward network\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        # Apply self attention\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "\n",
        "        # Add and normalize\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "\n",
        "        # Pass through the transformer block\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Set device and initialize word and position embeddings\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        # Create multiple decoder blocks\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        # Get sequence length and create position embeddings\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "        # Combine word embeddings and position embeddings\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        # Pass the input through the decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        # Apply the output linear layer\n",
        "        out = self.fc_out(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272d5a75",
      "metadata": {
        "id": "272d5a75"
      },
      "source": [
        "# Piecing Together the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Dy-4KTjy4Co",
      "metadata": {
        "id": "6Dy-4KTjy4Co"
      },
      "source": [
        "In this section, we combine our encoder and decoder to form the complete Transformer architecture. The encoder processes the input and the decoder uses this processed input to generate the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40decc8b",
      "metadata": {
        "id": "40decc8b"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"cpu\",\n",
        "        max_length=100,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Encoder to process the source sequence\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        # Decoder to generate the target sequence\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        # Define padding indices for source and target sequences\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # Create mask for source sequence to ignore padding tokens\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        # Create mask for target sequence to avoid attending to future tokens\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Create masks\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # Process source sequence with encoder\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # Generate target sequence with decoder\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d482d5",
      "metadata": {
        "id": "c3d482d5"
      },
      "source": [
        "# Testing the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
        "        device\n",
        "    )\n",
        "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "    src_pad_idx = 0\n",
        "    trg_pad_idx = 0\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_size = 10\n",
        "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
        "        device\n",
        "    )\n",
        "    out = model(x, trg[:, :-1])\n",
        "    print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpBmDs3qCjwt",
        "outputId": "f6310a3b-1264-4adc-c5e8-b2d9293dd480"
      },
      "id": "YpBmDs3qCjwt",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df7d192",
      "metadata": {
        "id": "4df7d192"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we've built a Transformer model from scratch, delved deep into its core components, and tested its capabilities. Through this exercise, we have gained a deeper understanding of how this powerful architecture works.\n",
        "\n",
        "Building the Transformer from scratch has given us insights into its design and functioning. The self-attention mechanism's ability to weigh the importance of different words in a sequence is a significant advantage, allowing Transformers to achieve state-of-the-art performance on various NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf26cfe5",
      "metadata": {
        "id": "bf26cfe5"
      },
      "source": [
        "# References\n",
        "\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}